{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = \"data_points\"\n",
    "if not os.path.exists(csv_path): os.makedirs(csv_path)\n",
    "log_path =  os.path.join(csv_path, 'logs')\n",
    "if not os.path.exists(log_path): os.makedirs(log_path)\n",
    "\n",
    "# Variables for OutlierRemover Class\n",
    "origin_csv_path = os.path.join(csv_path, 'data_points_origin.csv')\n",
    "outlier_removed_csv_path = os.path.join(csv_path, 'outlier_removed_data_points.csv')\n",
    "lower_outliers_path =   os.path.join(log_path, 'lower_outliers.csv')\n",
    "upper_outliers_path = os.path.join(log_path, 'upper_outliers.csv')\n",
    "\n",
    "# Variable for DataNormalizer Class\n",
    "normalized_csv_path = os.path.join(csv_path, 'normalized_data_points.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutlierRemover:\n",
    "    def __init__(self, filepath):\n",
    "        \"\"\"Initialize the DataFrame by taking the path to the CSV file\"\"\"\n",
    "        self.origin_data = pd.read_csv(filepath)\n",
    "    \n",
    "    def remove_outliers(self, column_index):\n",
    "        \"\"\"Calculates the IQR for the specified column index and removes rows with outliers\"\"\"\n",
    "        # Select that column\n",
    "        col_values = self.origin_data.iloc[:, column_index]\n",
    "        \n",
    "        # Calculate IQRs\n",
    "        Q1 = col_values.quantile(0.25)\n",
    "        Q3 = col_values.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        # Calculate outlier boundary values\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        # Filtering outliers\n",
    "        self.lower_outliers = self.origin_data[(col_values < lower_bound)]\n",
    "        self.upper_outliers = self.origin_data[(col_values > upper_bound)]\n",
    "        self.data = self.origin_data[(col_values >= lower_bound) & (col_values <= upper_bound)]\n",
    "\n",
    "    \n",
    "    def save_data(self, output_filepath):\n",
    "        self.data.to_csv(output_filepath, index=False)\n",
    "        print(f\"Outlier removed csv file was created in -> {output_filepath}\")\n",
    "\n",
    "    def save_outliers_data(self, lower_outlier_path, upper_outlier_path):\n",
    "        self.lower_outliers.to_csv(lower_outlier_path, index=False)\n",
    "        print(f\"Data detected as outliers stored in (lower bound) -> {lower_outlier_path}\")\n",
    "        self.upper_outliers.to_csv(upper_outlier_path, index=False)\n",
    "        print(f\"Data detected as outliers stored in (upper bound) -> {upper_outlier_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlier removed csv file was created in -> data_points/outlier_removed_data_points.csv\n",
      "Data detected as outliers stored in (lower bound) -> data_points/logs/lower_outliers.csv\n",
      "Data detected as outliers stored in (upper bound) -> data_points/logs/upper_outliers.csv\n"
     ]
    }
   ],
   "source": [
    "remover = OutlierRemover(origin_csv_path)\n",
    "remover.remove_outliers(column_index=3)  # Detect outliers based on the 4th index (run_time)\n",
    "remover.save_data(outlier_removed_csv_path)\n",
    "remover.save_outliers_data(lower_outliers_path, upper_outliers_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Origin data length : 1177\n",
      "Outliers removed data length : 1150\n",
      "Lower outliers length : 0\n",
      "Upper outliers length : 27\n"
     ]
    }
   ],
   "source": [
    "print( f\"Origin data length : {len(remover.origin_data)}\" )\n",
    "print( f\"Outliers removed data length : {len(remover.data)}\")\n",
    "print( f\"Lower outliers length : {len(remover.lower_outliers)}\")\n",
    "print( f\"Upper outliers length : {len(remover.upper_outliers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataNormalizer:\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "        self.data = self.load_data()\n",
    "\n",
    "    def load_data(self):\n",
    "        return pd.read_csv(self.filepath)\n",
    "\n",
    "    def normalize_columns(self, normalized_filepath, num_of_data=None):\n",
    "        \"\"\" Methods to normalize data from the 4th to the last column The data is scaled to values between 0 and 1 using the Min-Max normalization method\"\"\"\n",
    "        scaler = MinMaxScaler()\n",
    "        self.data.iloc[:, 3:] = scaler.fit_transform(self.data.iloc[:, 3:])\n",
    "        self.save_data(normalized_filepath, num_of_data)\n",
    "    \n",
    "    def save_data(self, normalized_filepath, num_of_data = None):\n",
    "\n",
    "        self.data.columns = ['x','y','z','execution_time','distance','joint_delta_6ea','joint_delta_3ea','joint1_delta','joint2_delta','joint3_delta','joint4_delta','joint5_delta','joint6_delta',\"average_node\"]\n",
    "        print(self.data)\n",
    "        if(num_of_data == None):\n",
    "            self.data.to_csv(normalized_filepath, index=False)\n",
    "        else:\n",
    "            sampled_data = self.data.sample(n=num_of_data, random_state=42)\n",
    "            sampled_data.to_csv(normalized_filepath, index=False)\n",
    "            print( f\"Number of data changes based on \\\"num_of_data\\\" : ( {len(self.data)} ) -> ( {num_of_data} )\" )\n",
    "            \n",
    "        print(f\"Normalized csv file was created in -> {normalized_filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             x         y         z  execution_time  distance  joint_delta_6ea  \\\n",
      "0    -0.576016  1.039328  1.406699        0.274924  0.448975         0.252998   \n",
      "1    -1.067565  0.501802  1.064382        0.734571  0.513439         0.733742   \n",
      "2     0.955757 -0.095256  1.139072        0.246871  0.442075         0.177250   \n",
      "3    -0.124110 -0.817203  0.966463        0.297799  0.457012         0.385626   \n",
      "4    -0.870976 -0.451888  1.038562        0.325421  0.472049         0.195902   \n",
      "...        ...       ...       ...             ...       ...              ...   \n",
      "1145 -0.385820 -0.937526  0.135363        0.422098  0.729757         0.420351   \n",
      "1146 -0.385820 -0.937526  0.135363        0.601208  0.729779         0.638815   \n",
      "1147  0.151257  0.099189  1.456115        0.805352  0.201551         0.741224   \n",
      "1148  0.151257  0.099189  1.456115        0.600777  0.201561         0.727047   \n",
      "1149  0.565434 -0.540548  1.421068        0.359517  0.334924         0.637666   \n",
      "\n",
      "      joint_delta_3ea  joint1_delta  joint2_delta  joint3_delta  joint4_delta  \\\n",
      "0            0.262467      0.167650      0.524171      0.286635      0.496038   \n",
      "1            0.751002      0.932456      0.231968      0.506630      0.499809   \n",
      "2            0.140547      0.017574      0.057739      0.610466      0.240643   \n",
      "3            0.472116      0.228574      0.758173      0.749754      0.504629   \n",
      "4            0.340295      0.077580      0.731836      0.641595      0.000076   \n",
      "...               ...           ...           ...           ...           ...   \n",
      "1145         0.444337      0.313945      0.390081      0.749319      0.751249   \n",
      "1146         0.687747      0.686820      0.390134      0.749344      0.744474   \n",
      "1147         0.823003      0.917402      0.386522      0.692135      0.499791   \n",
      "1148         0.616271      0.624659      0.317529      0.692075      0.238690   \n",
      "1149         0.188079      0.123213      0.033300      0.547510      0.500190   \n",
      "\n",
      "      joint5_delta  joint6_delta  average_node  \n",
      "0         0.121283      0.001620      0.324675  \n",
      "1         0.786211      0.499716      0.717532  \n",
      "2         0.003869      0.259333      0.785714  \n",
      "3         0.320729      0.000945      0.503247  \n",
      "4         0.272384      0.000102      0.774351  \n",
      "...            ...           ...           ...  \n",
      "1145      0.001571      0.254215      0.761364  \n",
      "1146      0.001550      0.758473      0.766234  \n",
      "1147      0.705340      0.500247      0.675325  \n",
      "1148      0.965927      0.760580      0.699675  \n",
      "1149      0.768437      0.995710      0.754870  \n",
      "\n",
      "[1150 rows x 14 columns]\n",
      "Normalized csv file was created in -> data_points/normalized_data_points.csv\n"
     ]
    }
   ],
   "source": [
    "normalizer = DataNormalizer(outlier_removed_csv_path) \n",
    "normalized_data = normalizer.normalize_columns(normalized_csv_path, num_of_data = None) #num_of_data can be used to determine the number of data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
